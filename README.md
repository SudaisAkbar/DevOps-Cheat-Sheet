1ï¸âƒ£ Editing Production Directly
Editing production servers directly (for example using nano) is a serious DevOps anti-pattern. It removes version control, meaning there is no history of changes, no accountability, and no easy rollback if something breaks. Changes go live without testing, which increases the risk of bugs reaching users and causing outages. It also creates security risks because direct server access is hard to audit and control.
The correct approach is to use Git as a single source of truth, where all changes are tracked and reviewed. A CI/CD pipeline should automatically test and deploy code in a controlled manner. Infrastructure should be managed using Infrastructure as Code, ensuring consistency across environments. Direct access to production servers should be restricted, and basic monitoring should be added to detect issues early. Automation and traceability together improve reliability.
________________________________________
2ï¸âƒ£ Traffic Spikes / Application Crashes
Applications that cannot handle traffic spikes usually rely on manual scaling, which is slow and unreliable. During high traffic, this leads to downtime, poor user experience, and potential revenue loss. Manual fixes do not scale and often happen too late.
From a DevOps perspective, systems should be designed to scale automatically. This requires horizontal scaling instead of vertical scaling, meaning adding more instances rather than upgrading a single server. The application should be stateless so that any instance can handle any request. Load balancing is needed to distribute traffic evenly. Autoscaling based on metrics such as CPU or request count ensures the system responds to demand in real time. Health checks and monitoring help remove unhealthy instances automatically. Modern systems must scale by design, not by human intervention.
________________________________________
3ï¸âƒ£ Secrets in Code or Git
Storing secrets such as API keys, passwords, or tokens in code or Git repositories is a major security risk. Once a secret is committed, it becomes part of the Git history and can be exposed permanently. Anyone with repository access can misuse these secrets, leading to data breaches or system compromise.
The correct DevOps practice is to never store secrets in version control. Secrets should be managed separately using secret managers or CI/CD environment variables. They should be injected at deployment or runtime rather than hard-coded. This allows secrets to be rotated without changing application code and reduces the blast radius of leaks. Access should follow the principle of least privilege. Treating secrets as configuration instead of code improves security, compliance, and operational safety.
________________________________________
4ï¸âƒ£ No Monitoring or Visibility
Without monitoring, failures are often detected by users instead of the engineering team. This leads to slow incident response, poor reliability, and difficult debugging because there is no clear insight into what went wrong. Operating systems without visibility turns troubleshooting into guesswork.
A proper DevOps setup includes observability through metrics, logs, and health checks. Metrics provide information about system health and performance, logs explain what happened inside the system, and health checks help detect failing services early. Dashboards give teams real-time visibility into system behavior, while alerts notify them before users are affected. Monitoring enables faster debugging, better scaling decisions, and higher system reliability. In DevOps, visibility is essential because problems cannot be fixed if they cannot be seen.
________________________________________
5ï¸âƒ£ Cattle vs. Pets
The â€œCattle vs. Petsâ€ analogy describes how servers are managed. Pets are individually maintained, irreplaceable, and require manual careâ€”if a pet server dies, itâ€™s a crisis. Cattle, by contrast, are disposable, identical, and replaceable; if one fails, it is destroyed and replaced automatically. This mindset is critical for modern DevOps because it enables autoscaling and fault tolerance. Stateless design is required, so that any server can handle requests without dependency on local data. Kubernetes exemplifies cattle management: pods can be created or destroyed automatically, with load balancing and health checks ensuring reliability. Treating infrastructure as cattle reduces operational overhead, supports rolling updates, and allows horizontal scaling without downtime. Modern cloud-native applications rely on this approach for resilience, automation, and predictable behavior.
________________________________________
6ï¸âƒ£ Idempotency
Idempotency ensures that performing an action multiple times results in the same outcome as doing it once. This property is crucial for automation and CI/CD pipelines because it allows retries without side effects, ensuring consistency and reliability. For example, kubectl apply or helm upgrade --install are idempotent: running them multiple times does not break the system. Non-idempotent operations, like incrementing counters or deleting files without checks, can cause inconsistent states. Idempotency makes deployments predictable and safe, reduces the risk of errors, and simplifies automated processes. In DevOps, automation must be reliable, repeatable, and safe to re-executeâ€”idempotency guarantees that pipelines and scripts behave consistently even when rerun, enabling confident continuous deployment.
________________________________________
7ï¸âƒ£ GitOps
GitOps is a DevOps approach where Git serves as the single source of truth for both application code and infrastructure. All changes are made via Git commits, which trigger automated deployments through CI/CD pipelines. This approach ensures that deployments are versioned, auditable, and reproducible. Rollbacks are easy because Git history records all changes. Infrastructure as code and declarative configurations, like Kubernetes manifests or Helm charts, enable GitOps workflows. By integrating GitOps, operations follow the same workflow as development: changes are reviewed, tested, and deployed automatically. GitOps increases reliability, reduces human error, improves collaboration, and provides traceability, making it a core best practice for modern, cloud-native DevOps environments.
________________________________________
8ï¸âƒ£ Shift Left
Shift Left is the practice of moving testing, security, and quality checks earlier in the development lifecycle. By detecting issues during development instead of production, feedback is faster, bugs are cheaper to fix, and overall software quality improves. For instance, running unit tests, linting, and security scans in early CI/CD stages prevents problems from reaching live environments. Security shift-left measures, such as scanning for secrets or vulnerabilities during the build, ensure that risks are mitigated proactively. By shifting left, teams catch errors before deployment, accelerate delivery, and reduce downtime. In DevOps, Shift Left reduces risk, increases efficiency, and aligns with the principle of continuous improvement and early validation.
________________________________________
9ï¸âƒ£ Observability
Observability is the ability to understand the internal state of a system based on its outputs. It relies on three pillars: metrics, logs, and traces. Metrics provide numeric measurements of system health, logs record events and errors, and traces show the flow of requests across distributed components. Observability enables teams to detect, diagnose, and fix issues faster. For example, Prometheus metrics can monitor latency, traffic, and error rates, while Grafana dashboards visualize system performance. Without observability, debugging is guesswork. With it, teams can proactively scale applications, improve reliability, and respond to failures effectively. Observability transforms operational data into actionable insights, supporting automation and high availability in DevOps workflows.
________________________________________
ðŸ”Ÿ Immutable Infrastructure
Immutable infrastructure means that servers or containers are never modified after deployment. Updates are performed by replacing instances with new ones rather than editing in place. This ensures consistency across environments, simplifies rollback, and reduces configuration drift. For example, Docker images built in CI/CD pipelines are immutable: any change requires building a new image, which is then deployed via Kubernetes. Rolling updates replace old pods with new ones without downtime, and previous versions can be restored easily. Immutable infrastructure reduces human errors, ensures predictable deployments, and aligns with DevOps principles like automation, reproducibility, and stateless design. â€œRebuild, donâ€™t repairâ€ is the key mindset.
________________________________________
1ï¸âƒ£1ï¸âƒ£ CI/CD Pipelines
CI/CD pipelines automate the process of building, testing, and deploying software. Continuous Integration ensures that code changes are automatically tested and integrated into the main branch, while Continuous Deployment automates release to production. Automation reduces human error, accelerates delivery, and allows safe rollbacks. Pipelines can include stages for linting, testing, security scans, building Docker images, and deployment. Tools like GitLab CI/CD or GitHub Actions orchestrate these stages. By standardizing the deployment process, CI/CD pipelines increase reliability, consistency, and feedback speed. Automation replaces manual work, ensuring that software is delivered faster, safer, and with predictable outcomes.
________________________________________
1ï¸âƒ£2ï¸âƒ£ Infrastructure as Code (IaC)
Infrastructure as Code (IaC) manages infrastructure using configuration files and code rather than manual processes. IaC ensures that environments are version-controlled, reproducible, and automated. Kubernetes manifests, Helm charts, and Terraform scripts are examples of IaC tools. By defining infrastructure declaratively, teams can create consistent staging, testing, and production environments, reducing drift and human error. CI/CD pipelines can deploy infrastructure automatically, enabling repeatable and reliable provisioning. IaC allows rollback to previous configurations and simplifies scaling. In DevOps, infrastructure should be reproducible, testable, and automated, ensuring predictable deployments and operational efficiency.
________________________________________
1ï¸âƒ£3ï¸âƒ£ Security in DevOps (DevSecOps)
Security in DevOps is continuous and integrated throughout the software lifecycle. Best practices include least privilege access, secret management, vulnerability scanning, and shift-left security checks. Secrets should never be committed to Git and should be injected securely at deployment. CI/CD pipelines can scan code and container images for vulnerabilities early. Role-Based Access Control (RBAC) and network policies limit access and reduce risk. By embedding security into all stages of development, DevSecOps ensures compliance, reduces incidents, and aligns with automation principles. Continuous monitoring, proactive scanning, and early detection make security a default, not an afterthought.
________________________________________
ðŸŸ¢ CATEGORY 3: RETROSPECTIVE & ANALYSIS
(Easy marks if answered maturely)
1ï¸âƒ£ Technical Stack (Keep It Short)
â€¢	Frontend framework
â€¢	Backend framework
â€¢	Database
â€¢	Containers
â€¢	CI/CD
â€¢	Orchestration
â€¢	Monitoring
________________________________________
2ï¸âƒ£ What Would You Change?
Good answers include:
â€¢	Simplify architecture
â€¢	Add more testing
â€¢	Use managed services
â€¢	Improve alerting
â€¢	Reduce operational overhead
ðŸ‘‰ Show learning, not perfection
________________________________________
3ï¸âƒ£ Statelessness
â€¢	No local storage
â€¢	External database
â€¢	External file storage
â€¢	Enables scaling and resilience
ðŸ‘‰ Stateless apps scale and recover easily

2ND ANSWERS


# ðŸŽ¯ DevOps Exam - Complete Cheat Sheet
**Exam: 30.12.2025 | 14:00-16:00 | 6 Questions Ã— 5 Points = 30 Points | 90 Minutes**

> **Note**: This cheat sheet covers general DevOps concepts (Categories 1 & 2) and project-specific details (Category 3). File paths and project examples are included where relevant.

---

## ðŸ“‹ EXAM FORMAT

- **Format**: 6 freeform questions, 5 points each (30 points total)
- **Duration**: 90 minutes
- **Allowed**: Course materials, GitLab repos, personal notes
- **Forbidden**: Chatbots, communication, Google Search

### Question Categories:
1. **Applied DevOps Scenarios** (2 Â± 1 questions) - Solve real-world problems
2. **DevOps Concepts** (2 Â± 1 questions) - Explain theoretical foundations
3. **Retrospective & Analysis** (2 Â± 1 questions) - Reflect on your project

---

## ðŸŽ¯ CATEGORY 1: APPLIED DEVOPS SCENARIOS

### Scenario 1: Direct Production Edits (nano/SSH)
**Problem**: Developers edit production directly using `nano`/SSH

**Why Bad**:
- âŒ No version control â†’ can't track/rollback changes
- âŒ No testing â†’ bugs go to production
- âŒ No code review â†’ security risks
- âŒ No audit trail â†’ can't track who changed what
- âŒ Direct server access â†’ security vulnerability

**First Steps**:
1. **Immediate**: Set up Git repository, move all code to version control
2. **CI/CD Pipeline**: Create CI/CD pipeline with stages: lint â†’ test â†’ build â†’ deploy
3. **Infrastructure as Code**: Use Kubernetes manifests or Helm charts instead of manual edits
4. **Access Control**: Remove SSH access, require all changes via Git
5. **Monitoring**: Add health checks (`/healthz`, `/readyz`) and metrics (Prometheus) to detect issues early

**Key Points**:
- CI/CD pipeline should have: lint â†’ test â†’ security â†’ build â†’ container-build â†’ deploy
- Use Helm for templating (separates logic from configuration)
- Health probes in Kubernetes deployments
- Container registry for images

---

### Scenario 2: Application Crashes on Traffic Spikes
**Problem**: App can't handle traffic spikes, needs autoscaling

**Solution - HPA (Horizontal Pod Autoscaler)**:
- **Configuration**: Min 2, Max 5 replicas, Target CPU 30%
- **How It Works**: Kubernetes monitors CPU, scales up when >30%, down when <30%
- **Metrics**: Uses Prometheus metrics (`/metrics` endpoint, port 9090)

**Additional Steps**:
1. **Health Probes**: Liveness (`/healthz`) and Readiness (`/readyz`) probes
2. **Stateless Design**: App must be stateless (use external database/storage, not local filesystem)
3. **Load Balancing**: Kubernetes Service distributes traffic across replicas
4. **Monitoring**: Grafana dashboard to visualize scaling

**Key Points**:
- HPA enabled in production with pod anti-affinity for fault tolerance
- Metrics exposed on port 9090 with Prometheus annotations
- Application must be stateless to scale horizontally

---

### Scenario 3: Secrets in Git Repository
**Problem**: Secrets hardcoded in Git, security risk

**Solution - Secrets Management**:
1. **Remove from Git**: Delete all secrets from codebase
2. **CI/CD Variables**: Store secrets in CI/CD variables (encrypted, masked)
3. **Injection at Deploy**: Use templating (Helm `--set-string`) to inject secrets at deploy time
4. **Kubernetes Secrets**: Secrets created in K8s, never in Git
5. **Rotation**: Easy to rotate (update CI/CD variable, redeploy)

**Implementation Flow**:
- Secrets in CI/CD variables: `DATABASE_URL`, `JWT_SECRET`, `MINIO_ACCESS_KEY`, etc.
- Templating command: `--set-string serverSecret.extra.DATABASE_URL="${DATABASE_URL}"`
- Kubernetes Secret created, pods mount as env vars
- **Never in Git**: Values files have empty placeholders, secrets only in CI/CD

---

### Scenario 4: No Monitoring/Visibility
**Problem**: Can't see what's happening in production

**Solution - Observability**:
1. **Metrics**: Prometheus metrics (4 golden signals)
2. **Health Checks**: Liveness and Readiness probes
3. **Logging**: Application logs via `kubectl logs`
4. **Visualization**: Grafana dashboards

**Implementation**:
- **Metrics Service**: Expose Prometheus metrics endpoint
- **Metrics**: `http_request_duration_seconds`, `http_requests_total`, `http_errors_total`
- **Health**: `/healthz` (liveness), `/readyz` (readiness, checks DB)
- **Prometheus**: Port 9090, Service annotations `prometheus.io/scrape: "true"`
- **Grafana**: Dashboards with panels (latency, errors, CPU, pod health, HPA)

---

### Scenario 5: Manual Deployments
**Problem**: Deployments done manually, error-prone

**Solution - CI/CD Pipeline**:
1. **Automate**: CI/CD pipeline (e.g., GitLab CI/CD)
2. **Stages**: Lint â†’ Test â†’ Security â†’ Build â†’ Container â†’ Scan â†’ Deploy
3. **Atomic Deployments**: `--atomic` flag for automatic rollback
4. **Zero-Downtime**: Rolling updates (new pods ready before old removed)

**Pipeline Stages**:
- **Lint**: Code quality checks
- **Test**: Unit tests with coverage requirements
- **Security**: Secret detection, vulnerability scanning
- **Build**: Application builds
- **Container Build**: Docker images to registry
- **Image Scan**: Security scanning (Trivy)
- **Deploy**: Helm to Kubernetes, atomic with rollback

---

## ðŸ§  CATEGORY 2: DEVOPS CONCEPTS

### Concept 1: Cattle vs. Pets
**Definition**: 
- **Pets**: Named servers, manually maintained, irreplaceable (if dies = crisis)
- **Cattle**: Numbered servers, identical, easily replaced (if dies = replace it)

**How It Influences Autoscaling**:
- **Stateless Design**: Apps must be stateless (external database/storage, not local filesystem)
- **Immutable Infrastructure**: Containers built once, never modified (Docker images in CI/CD)
- **Horizontal Scaling**: Add/remove identical pods (HPA: 2-5 replicas)
- **No Manual Intervention**: Pods created/destroyed automatically (Kubernetes)
- **Fault Tolerance**: Pod crashes â†’ Kubernetes creates new one (liveness probe)

**Key Points**:
- Stateless applications enable horizontal scaling
- Identical pods can be replaced without data loss
- HPA automatically creates/destroys pods based on metrics
- Pod anti-affinity ensures pods spread across nodes

---

### Concept 2: Idempotency
**Definition**: Operation produces same result whether executed once or multiple times.

**Why Important**:
- **Safe Retries**: Deployment fails â†’ retry without side effects
- **Consistency**: Same config applied multiple times = same result
- **CI/CD Reliability**: Pipeline re-run without breaking things

**Examples**:
- âœ… `helm upgrade --install` (idempotent - can run multiple times)
- âœ… `kubectl apply` (idempotent - creates if missing, updates if exists)
- âœ… Database migrations (idempotent - safe to run multiple times)
- âœ… Docker builds (same image = same result)

**Non-Idempotent (Bad)**:
- âŒ Deleting file (can't delete twice)
- âŒ Incrementing counter (different result each time)

---

### Concept 3: GitOps
**Definition**: Infrastructure and code in Git, Git = single source of truth. Changes via Git commits, not manual commands.

**Principles**:
1. **Declarative**: Infrastructure in YAML (Kubernetes manifests, Helm charts)
2. **Version Controlled**: All config in Git (e.g., `k8s/`, `helm/` directories)
3. **Automated**: CI/CD applies changes (e.g., `.gitlab-ci.yml` deploys on `main`)
4. **Observable**: Monitor what's deployed (Grafana dashboard)

**Implementation**:
- **Infrastructure as Code**: All K8s resources in version control
- **CI/CD Pipeline**: Auto-deploys on push to main branch
- **No Manual Changes**: All deployments via Git commits, not manual commands
- **Rollback**: Git history allows reverting to previous versions
- **Secrets**: Injected from CI/CD variables, not in Git

**Benefits**:
- Audit trail (who changed what, when)
- Easy rollback (revert Git commit)
- Consistency (same config in staging/production)

---

### Concept 4: Shift Left
**Definition**: Move testing, security, quality checks earlier in development lifecycle (left = earlier in timeline).

**Traditional (Bad)**: Test and fix issues in production
**Shift Left (Good)**: Test and fix issues during development

**CI/CD Pipeline Stages (Shift Left)**:
1. **Lint Stage** (earliest): Code quality checks, catches syntax errors early
2. **Test Stage**: Unit tests run before deployment, coverage requirements, failures block deployment
3. **Security Stage**: Scans for hardcoded secrets, catches security issues early
4. **Build Stage**: Builds fail fast if code doesn't compile, catches dependency issues early
5. **Image Scan Stage**: Scans Docker images for vulnerabilities
6. **Deploy Stage** (last): Only deploys if all previous stages pass

**Benefits**:
- Faster feedback (find issues in minutes, not days)
- Lower cost (fixing bugs in dev cheaper than production)
- Better quality (catch issues before users see them)

---

### Concept 5: Observability (Metrics/Logs/Traces)
**Definition**: Three pillars of observability

**Three Pillars**:
1. **Metrics**: Numerical measurements over time (CPU, memory, request rate)
2. **Logs**: Text records of events (application logs, error messages)
3. **Traces**: Request flow through distributed systems

**Implementation**:

**Metrics (Prometheus)**:
- **Four Golden Signals**:
  - **Latency**: `http_request_duration_seconds` (histogram, buckets 0.1s-10s)
  - **Traffic**: `http_requests_total` (counter, labels: method, route, status)
  - **Errors**: `http_errors_total` (counter, status >= 400)
  - **Saturation**: Custom metrics (e.g., `posts_total`, `users_total`)
- **Exposure**: `/metrics` endpoint on port 9090
- **Scraping**: Prometheus scrapes via Service annotations (`prometheus.io/scrape: "true"`)

**Logs**:
- Application logs via `kubectl logs`
- Structured logging

**Health Checks**:
- **Liveness Probe**: `/healthz` (is app running?)
- **Readiness Probe**: `/readyz` (is app ready for traffic? checks database)

**Visualization**: Grafana dashboard with panels (latency, errors, CPU, pod health, etc.)

---

### Concept 6: Immutable Infrastructure
**Definition**: Servers/containers never modified after creation. To update, create new containers and destroy old ones.

**Traditional (Mutable)**: SSH into server, edit files, install packages â†’ server drifts from original state
**Immutable**: Build new container image, deploy it, destroy old containers â†’ always consistent

**How Containerization Enables It**:
- **Docker Images**: Built once in CI/CD (e.g., `build-server-image`, `build-client-image` jobs)
- **No Manual Changes**: Containers are read-only (except volumes)
- **Version Control**: Images tagged with Git commit (e.g., `IMAGE_TAG: $CI_COMMIT_REF_SLUG`)
- **Rolling Updates**: Kubernetes replaces old pods with new ones (zero-downtime)

**Benefits**:
- Consistency (dev = staging = production)
- Reproducibility (same image = same behavior)
- Easy rollback (deploy previous image)
- Security (no manual changes = fewer vulnerabilities)

---

### Concept 7: CI/CD Pipelines
**Definition**: Automated pipeline that builds, tests, and deploys code.

**Pipeline Stages** (example):

1. **Lint Stage**: ESLint checks code, catches syntax/style errors early
2. **Test Stage**: Unit tests with coverage requirements, ensures code works before deployment
3. **Security Stage**: Scans for hardcoded secrets, prevents secrets in Git
4. **Build Stage**: Creates build artifacts, artifacts cached
5. **Container Build Stage**: Builds Docker images, pushes to registry, images tagged with commit SHA
6. **Image Scan Stage**: Scans images for HIGH/CRITICAL vulnerabilities
7. **Deploy Stage**: Deploys to Kubernetes using Helm, creates ConfigMaps, Secrets, Deployments, Services, Ingress

**Key Features**:
- **Atomic Deployments**: `--atomic` flag rolls back on failure
- **Secrets Injection**: Secrets from CI/CD variables, not in Git
- **Zero-Downtime**: Rolling updates (new pods ready before old ones removed)

---

### Concept 8: Version Control
**Definition**: System for tracking changes to files over time.

**Why Important**:
- **History**: See who changed what, when, why
- **Rollback**: Revert to previous versions
- **Collaboration**: Multiple developers work on same code
- **Branching**: Test changes without affecting main code

**Best Practices**:
- Commit frequently with clear messages
- Use branches for features
- Never commit secrets
- Use pull requests for code review

---

### Concept 9: Infrastructure as Code (IaC)
**Definition**: Managing infrastructure using code/configuration files instead of manual processes.

**Benefits**:
- **Version Control**: Infrastructure changes tracked in Git
- **Reproducibility**: Same config = same infrastructure
- **Automation**: CI/CD can deploy infrastructure
- **Consistency**: Dev = staging = production

**Tools**:
- **Kubernetes**: Declarative YAML manifests
- **Helm**: Templating engine for K8s (separates logic from config)
- **Docker Compose**: Local development orchestration

---

### Concept 10: Security in DevOps
**Definition**: Integrating security practices throughout the development lifecycle.

**Security Measures**:

1. **Secret Management**:
   - Secrets in CI/CD variables (encrypted, masked)
   - Never in Git
   - Injected at deploy time via templating

2. **Security Scanning**:
   - **Secret Detection**: Scans for hardcoded secrets in Git
   - **Image Scanning**: Scans Docker images for vulnerabilities (HIGH/CRITICAL)

3. **Access Control**:
   - RBAC in Kubernetes
   - ServiceAccount for CI/CD
   - No direct SSH access to containers

4. **Network Security**:
   - NetworkPolicy for traffic control
   - Ingress with TLS (cert-manager, Let's Encrypt)

5. **Application Security**:
   - JWT authentication
   - Password hashing (bcrypt)
   - Input validation
   - XSS prevention

**Shift Left Security**: Security checks early in pipeline (secret detection, image scanning)

---

## ðŸ” CATEGORY 3: RETROSPECTIVE & ANALYSIS

> **Note**: This section contains project-specific details. Replace with your own project information.

### Your Project Summary

**Technical Stack**:
- **Frontend**: Next.js (React), TypeScript, Tailwind CSS
- **Backend**: NestJS (Node.js), TypeScript, Prisma ORM
- **Database**: PostgreSQL (Kubernetes deployment)
- **Storage**: MinIO (S3-compatible object storage)
- **Containerization**: Docker (multi-stage builds)
- **Orchestration**: Kubernetes (Deployments, Services, Ingress, PVCs)
- **Templating**: Helm (separates logic from configuration)
- **CI/CD**: GitLab CI/CD (7 stages: lint â†’ test â†’ security â†’ build â†’ container-build â†’ image-scan â†’ deploy)
- **Monitoring**: Prometheus (metrics), Grafana (dashboards)
- **Security**: Trivy (image scanning), gitleaks (secret detection)

**Architecture Decisions**:
- **Stateless Design**: No local filesystem storage (PostgreSQL + MinIO external)
- **Health Probes**: Liveness (`/healthz`) and Readiness (`/readyz`)
- **Autoscaling**: HPA (2-5 replicas, CPU-based, production only)
- **Fault Tolerance**: Pod anti-affinity (pods on different nodes)
- **Secrets Management**: GitLab CI variables â†’ Helm â†’ Kubernetes Secrets
- **Templating**: Helm (values.yaml for staging, values-prod.yaml for production)

---

### Question: "What technical stack did you use? What would you change?"

**Current Stack**:
- **Frontend**: Next.js (good for SSR, but adds complexity)
- **Backend**: NestJS (excellent for large apps, but might be overkill for simple CRUD)
- **Database**: PostgreSQL (perfect choice, relational data)
- **Storage**: MinIO (good S3-compatible, but AWS S3 might be better for production)
- **Orchestration**: Kubernetes + Helm (industry standard, but complex for small teams)
- **CI/CD**: GitLab CI (good, but could add more stages like integration tests)

**What I Would Change**:

1. **Simplify Frontend**: Consider React without Next.js if SSR not needed (faster builds)
2. **Add Integration Tests**: E2E tests in CI/CD pipeline (currently only unit tests)
3. **Use Managed Services**: Consider managed PostgreSQL (AWS RDS) instead of self-hosted
4. **Add Monitoring Alerts**: Set up Prometheus alerts (currently only dashboards)
5. **Improve Security**: Add OWASP dependency scanning (currently only Trivy for images)
6. **Optimize Images**: Use multi-stage builds more aggressively (reduce image size)
7. **Add Blue-Green Deployment**: For zero-downtime (currently only rolling updates)

**Why These Changes**:
- **Managed Services**: Less operational overhead, better reliability
- **More Testing**: Catch integration issues before production
- **Alerts**: Proactive issue detection (not just reactive dashboards)
- **Smaller Images**: Faster deployments, lower costs

---

### Question: "How did you ensure your application is stateless? Why is this important?"

**How We Ensured Statelessness**:

1. **Database Externalization**:
   - All data in PostgreSQL (Kubernetes service, not local)
   - ORM connects to service name (e.g., `postgresql:5432`, not localhost)
   - Data persists in PersistentVolumeClaim (survives pod restarts)

2. **File Storage Externalization**:
   - Images uploaded to MinIO (S3-compatible)
   - Storage service uses `PutObject`/`GetObject` (never writes to `/tmp`)
   - Files accessible from any pod (shared storage)

3. **Session Management**:
   - JWT tokens stored client-side (browser localStorage)
   - Sessions in database (not in-memory)
   - No pod-local session storage

4. **No Local State**:
   - No files written to container filesystem
   - No in-memory caches (all data from database/storage)

**Why Important**:
- **Horizontal Scaling**: Multiple pods can serve requests (no data inconsistency)
- **Pod Failures**: If pod crashes, another pod takes over (no data loss)
- **Rolling Updates**: Zero-downtime deployments (old pods removed after new ones ready)
- **Chaos Engineering**: Application survives random pod kills

**Verification**:
- Pods can be deleted and recreated without data loss
- HPA scales to multiple replicas, all serve same data
- Rolling updates work without downtime

---

### Question: "Explain your secrets management strategy. Why is it secure?"

**Strategy**: **Never commit secrets to Git**. All secrets stored in CI/CD variables, injected only during deployment.

**Implementation**:

1. **Storage**:
   - Secrets in CI/CD variables (encrypted at rest, masked in logs)
   - Variables: `DATABASE_URL`, `JWT_SECRET`, `MINIO_ACCESS_KEY`, `MINIO_SECRET_KEY`, `NEXTAUTH_SECRET`

2. **Injection**:
   - Templating uses empty placeholders in values files (not in Git)
   - CI/CD job passes secrets via `--set-string` flags
   - Secrets only exist in: CI/CD variables â†’ CI job â†’ Kubernetes Secrets â†’ Pods

3. **Security Measures**:
   - **Masked**: Secrets masked in CI/CD logs (can't see values)
   - **Protected**: Variables marked as "protected" (only available in protected branches)
   - **Base64 Encoding**: Kubernetes Secrets base64-encoded (not plaintext in etcd)
   - **No Git History**: Secrets never in Git, so no risk of exposure in history

**Why Secure**:
- **No Git Exposure**: Even if repo is public, secrets aren't exposed
- **Access Control**: Only CI/CD job can access secrets (not developers)
- **Audit Trail**: CI/CD logs who accessed secrets (if enabled)
- **Rotation**: Easy to rotate (update CI/CD variable, redeploy)

**Example Flow**:
1. Developer pushes code (no secrets in code)
2. CI/CD job reads `JWT_SECRET` from CI/CD variables
3. Templating command: `--set-string serverSecret.extra.JWT_SECRET="${JWT_SECRET}"`
4. Kubernetes Secret created (base64-encoded)
5. Pod mounts secret as environment variable

---

### Question: "How did you implement monitoring? What metrics do you track?"

**Implementation**:

1. **Metrics Service** (e.g., `server/src/metrics/metrics.service.ts`):
   - **Four Golden Signals**:
     - **Latency**: `http_request_duration_seconds` (histogram, buckets: 0.1s-10s)
     - **Traffic**: `http_requests_total` (counter, labels: method, route, status)
     - **Errors**: `http_errors_total` (counter, status >= 400)
     - **Saturation**: Custom metrics (e.g., `posts_total`, `users_total`)
   - **Exposure**: Separate server on port 9090, `/metrics` endpoint
   - **Format**: Prometheus text format

2. **Metrics Middleware**:
   - Intercepts all HTTP requests
   - Records duration, method, route, status code
   - Calls metrics service

3. **Kubernetes Integration**:
   - **Service Annotations**: `prometheus.io/scrape: "true"`, `prometheus.io/port: "9090"`
   - **NetworkPolicy**: Allows Prometheus from monitoring namespace
   - **Scraping**: Prometheus scrapes every 30 seconds

4. **Health Checks**:
   - **Liveness**: `/healthz` (app running?)
   - **Readiness**: `/readyz` (app ready for traffic? checks database)

5. **Visualization**:
   - **Grafana Dashboard**: Multiple panels (latency, errors, CPU, pod health, HPA, etc.)
   - **Thresholds**: Color-coded (green/yellow/orange/red)
   - **Time Range**: Configurable (e.g., 6 hours default.



### Question: "Why did you choose Helm over Kustomize? (or vice versa)"

**Helm vs. Kustomize**:

**Helm**:
- **Pros**:
  - Templating engine (Go templates)
  - Values files (separates logic from config)
  - Package manager (can share charts)
  - Atomic deployments (`--atomic` flag)
  - Industry standard, large community
- **Cons**:
  - More complex (templates can be hard to read)
  - Requires Helm binary
  - Steeper learning curve

**Kustomize**:
- **Pros**:
  - Native to Kubernetes (built into `kubectl`)
  - Simpler (YAML overlays)
  - No external tools needed
  - Easy to understand
- **Cons**:
  - Less powerful (no templating)
  - Harder to share/reuse
  - No atomic deployments

**Why I Chose Helm**:
- **Templating**: Need to separate logic from config (values.yaml vs templates)
- **Multiple Environments**: Staging and production (values.yaml vs values-prod.yaml)
- **Secrets Injection**: Easy to inject secrets via `--set-string`
- **Atomic Deployments**: `--atomic` flag for automatic rollback
- **Industry Standard**: More widely used, better documentation





-



